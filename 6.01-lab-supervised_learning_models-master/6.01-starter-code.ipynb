{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./401ksubs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>15.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0000</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>37.155</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>3.483</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1380.4940</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>31.896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1017.3550</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>47.295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>5.290</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2236.8170</td>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>29.100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>29.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>846.8100</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>23.457</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>550.2309</td>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>31.785</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>18.149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1010.2860</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>34.941</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1220.8740</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>24.432</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>596.9226</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>25.131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>631.5672</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>19.074</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>363.8174</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>38.772</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>4.150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1503.2680</td>\n",
       "      <td>2209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>12.480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155.7504</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>45.390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>122.500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2060.2520</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>39.861</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1588.8990</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>102.600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>40.999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10526.7600</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>39.579</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>12.175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1566.4970</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>40.194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>8.300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1615.5580</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>25.254</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>9.687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>637.7645</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>10.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116.6400</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>27.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.0000</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>17.856</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>-15.495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>318.8368</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>38.940</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1516.3230</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>12.240</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>149.8176</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>18.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>324.0000</td>\n",
       "      <td>2116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9245</th>\n",
       "      <td>0</td>\n",
       "      <td>14.415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>8.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>207.7922</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9246</th>\n",
       "      <td>0</td>\n",
       "      <td>39.321</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1546.1410</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9247</th>\n",
       "      <td>0</td>\n",
       "      <td>32.631</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1064.7820</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9248</th>\n",
       "      <td>0</td>\n",
       "      <td>12.960</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167.9616</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>1</td>\n",
       "      <td>46.929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>17.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2202.3310</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>1</td>\n",
       "      <td>84.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>111.768</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7056.0000</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>0</td>\n",
       "      <td>28.059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>787.3075</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>1</td>\n",
       "      <td>49.572</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>5.650</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2457.3830</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>1</td>\n",
       "      <td>55.602</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10.300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3091.5830</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>1</td>\n",
       "      <td>43.920</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>9.600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1928.9660</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9255</th>\n",
       "      <td>1</td>\n",
       "      <td>26.430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>3.399</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>698.5449</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9256</th>\n",
       "      <td>1</td>\n",
       "      <td>35.385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1252.0980</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9257</th>\n",
       "      <td>0</td>\n",
       "      <td>29.700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>882.0900</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9258</th>\n",
       "      <td>0</td>\n",
       "      <td>28.572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816.3592</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9259</th>\n",
       "      <td>1</td>\n",
       "      <td>57.282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>28.579</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3281.2280</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9260</th>\n",
       "      <td>0</td>\n",
       "      <td>73.458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>9.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5396.0780</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>0</td>\n",
       "      <td>48.351</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2337.8190</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9262</th>\n",
       "      <td>0</td>\n",
       "      <td>21.675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469.8056</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9263</th>\n",
       "      <td>0</td>\n",
       "      <td>47.760</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>73.509</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2281.0170</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9264</th>\n",
       "      <td>0</td>\n",
       "      <td>46.113</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2126.4090</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>0</td>\n",
       "      <td>13.821</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>191.0201</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9266</th>\n",
       "      <td>1</td>\n",
       "      <td>37.275</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1389.4260</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>0</td>\n",
       "      <td>16.968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>69.200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>287.9130</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0</td>\n",
       "      <td>57.915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>3.067</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3354.1470</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>0</td>\n",
       "      <td>23.940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>573.1236</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>0</td>\n",
       "      <td>58.428</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3413.8310</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>0</td>\n",
       "      <td>24.546</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.5061</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>0</td>\n",
       "      <td>38.550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1486.1020</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>0</td>\n",
       "      <td>34.410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>3.550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1184.0480</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>0</td>\n",
       "      <td>25.608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>655.7697</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9275 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k      inc  marr  male  age  fsize   nettfa  p401k  pira  \\\n",
       "0         0   13.170     0     0   40      1    4.575      0     1   \n",
       "1         1   61.230     0     1   35      1  154.000      1     0   \n",
       "2         0   12.858     1     0   44      2    0.000      0     0   \n",
       "3         0   98.880     1     1   44      2   21.800      0     0   \n",
       "4         0   22.614     0     0   53      1   18.450      0     0   \n",
       "5         0   15.000     1     0   60      3    0.000      0     0   \n",
       "6         0   37.155     1     0   49      5    3.483      0     1   \n",
       "7         0   31.896     1     0   38      5   -2.100      0     0   \n",
       "8         0   47.295     1     0   52      2    5.290      0     1   \n",
       "9         1   29.100     0     1   45      1   29.600      0     1   \n",
       "10        0   23.457     1     0   61      3    0.000      0     0   \n",
       "11        0   31.785     1     0   40      6   18.149      0     0   \n",
       "12        0   34.941     1     0   48      3    0.695      0     0   \n",
       "13        0   24.432     1     0   60      2    0.200      0     0   \n",
       "14        0   25.131     1     0   43      5   -4.250      0     0   \n",
       "15        0   19.074     0     1   43      1    0.000      0     0   \n",
       "16        1   38.772     1     0   47      2    4.150      1     0   \n",
       "17        1   12.480     1     0   27      2  -10.000      0     0   \n",
       "18        1   45.390     1     0   57      2  122.500      0     1   \n",
       "19        0   39.861     1     0   35      2    1.600      0     1   \n",
       "20        1  102.600     1     0   53      5   40.999      1     1   \n",
       "21        0   39.579     1     0   36      4   12.175      0     0   \n",
       "22        0   40.194     1     0   40      3    8.300      0     1   \n",
       "23        0   25.254     0     1   31      1    9.687      0     1   \n",
       "24        0   10.800     0     0   48      2    0.500      0     0   \n",
       "25        0   27.000     0     1   42      1    0.130      0     0   \n",
       "26        0   17.856     1     0   42      5  -15.495      0     0   \n",
       "27        1   38.940     1     0   35      2    0.200      1     0   \n",
       "28        0   12.240     0     1   32      2   -2.500      0     1   \n",
       "29        0   18.000     0     0   46      2    0.000      0     1   \n",
       "...     ...      ...   ...   ...  ...    ...      ...    ...   ...   \n",
       "9245      0   14.415     0     1   38      1    8.600      0     1   \n",
       "9246      0   39.321     1     0   29      2    8.400      0     1   \n",
       "9247      0   32.631     1     0   37      4   -5.101      0     0   \n",
       "9248      0   12.960     0     1   28      1    0.000      0     0   \n",
       "9249      1   46.929     1     0   37      4   17.250      0     1   \n",
       "9250      1   84.000     1     0   48      2  111.768      1     0   \n",
       "9251      0   28.059     0     0   25      1    0.350      0     0   \n",
       "9252      1   49.572     1     1   33      4    5.650      1     0   \n",
       "9253      1   55.602     1     1   32      4   10.300      0     0   \n",
       "9254      1   43.920     0     1   44      1    9.600      1     1   \n",
       "9255      1   26.430     0     0   30      1    3.399      1     0   \n",
       "9256      1   35.385     1     1   29      3    0.500      0     0   \n",
       "9257      0   29.700     1     0   56      3   -0.250      0     0   \n",
       "9258      0   28.572     0     1   28      1   -0.180      0     0   \n",
       "9259      1   57.282     0     0   36      1   28.579      1     0   \n",
       "9260      0   73.458     0     0   30      1    9.900      0     0   \n",
       "9261      0   48.351     1     0   36      4   -0.351      0     0   \n",
       "9262      0   21.675     0     0   28      2    4.500      0     0   \n",
       "9263      0   47.760     1     0   35      5   73.509      0     1   \n",
       "9264      0   46.113     1     0   44      5   -7.299      0     0   \n",
       "9265      0   13.821     0     1   36      2    0.050      0     0   \n",
       "9266      1   37.275     1     0   37      6    1.180      1     1   \n",
       "9267      0   16.968     0     0   49      1   69.200      0     1   \n",
       "9268      0   57.915     1     0   45      3    3.067      0     0   \n",
       "9269      0   23.940     1     1   29      3    0.350      0     0   \n",
       "9270      0   58.428     1     0   33      4   -1.200      0     0   \n",
       "9271      0   24.546     0     1   37      3    2.000      0     0   \n",
       "9272      0   38.550     1     0   33      3  -13.600      0     1   \n",
       "9273      0   34.410     1     0   57      3    3.550      0     0   \n",
       "9274      0   25.608     0     1   49      1    1.800      0     0   \n",
       "\n",
       "           incsq  agesq  \n",
       "0       173.4489   1600  \n",
       "1      3749.1130   1225  \n",
       "2       165.3282   1936  \n",
       "3      9777.2540   1936  \n",
       "4       511.3930   2809  \n",
       "5       225.0000   3600  \n",
       "6      1380.4940   2401  \n",
       "7      1017.3550   1444  \n",
       "8      2236.8170   2704  \n",
       "9       846.8100   2025  \n",
       "10      550.2309   3721  \n",
       "11     1010.2860   1600  \n",
       "12     1220.8740   2304  \n",
       "13      596.9226   3600  \n",
       "14      631.5672   1849  \n",
       "15      363.8174   1849  \n",
       "16     1503.2680   2209  \n",
       "17      155.7504    729  \n",
       "18     2060.2520   3249  \n",
       "19     1588.8990   1225  \n",
       "20    10526.7600   2809  \n",
       "21     1566.4970   1296  \n",
       "22     1615.5580   1600  \n",
       "23      637.7645    961  \n",
       "24      116.6400   2304  \n",
       "25      729.0000   1764  \n",
       "26      318.8368   1764  \n",
       "27     1516.3230   1225  \n",
       "28      149.8176   1024  \n",
       "29      324.0000   2116  \n",
       "...          ...    ...  \n",
       "9245    207.7922   1444  \n",
       "9246   1546.1410    841  \n",
       "9247   1064.7820   1369  \n",
       "9248    167.9616    784  \n",
       "9249   2202.3310   1369  \n",
       "9250   7056.0000   2304  \n",
       "9251    787.3075    625  \n",
       "9252   2457.3830   1089  \n",
       "9253   3091.5830   1024  \n",
       "9254   1928.9660   1936  \n",
       "9255    698.5449    900  \n",
       "9256   1252.0980    841  \n",
       "9257    882.0900   3136  \n",
       "9258    816.3592    784  \n",
       "9259   3281.2280   1296  \n",
       "9260   5396.0780    900  \n",
       "9261   2337.8190   1296  \n",
       "9262    469.8056    784  \n",
       "9263   2281.0170   1225  \n",
       "9264   2126.4090   1936  \n",
       "9265    191.0201   1296  \n",
       "9266   1389.4260   1369  \n",
       "9267    287.9130   2401  \n",
       "9268   3354.1470   2025  \n",
       "9269    573.1236    841  \n",
       "9270   3413.8310   1089  \n",
       "9271    602.5061   1369  \n",
       "9272   1486.1020   1089  \n",
       "9273   1184.0480   3249  \n",
       "9274    655.7697   2401  \n",
       "\n",
       "[9275 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k     1.000000\n",
       "inc       0.268178\n",
       "marr      0.080843\n",
       "male     -0.027641\n",
       "age       0.031526\n",
       "fsize     0.012015\n",
       "nettfa    0.143950\n",
       "p401k     0.769170\n",
       "pira      0.118643\n",
       "incsq     0.206618\n",
       "agesq     0.017526\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corrwith(df['e401k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Advertising based on race is implicite Racism, it assumes a trend across all members of a certain race and allows for racist decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#income, marriage and nettfa are likely the best variables (based on the above correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income Sq and Age Sq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age Sq?  Seems like a inneffective adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor, Decision Tree, Linear Regression, Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a $k$-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['e401k','p401k','pira','inc'], axis = 1)\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.fit(X_train,y_train)\n",
    "svr.score(X_test,y_test)\n",
    "svr_preds = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test,y_test)\n",
    "knn_preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.fit(X_train,y_train)\n",
    "bag.score(X_test,y_test)\n",
    "\n",
    "bag_preds = bag.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test)\n",
    "lr_preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_train,y_train)\n",
    "rfr.score(X_test,y_test)\n",
    "\n",
    "rfr_preds = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(X_train,y_train)\n",
    "ada.score(X_test,y_test)\n",
    "ada_preds = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Its a way of random sampling our data WITH replacement (i.e in our random sample we do not remove the samples we select, so there is possiblility for multiples in our sets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Essentially the bag makes a cluster of models at random and test their choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All the features are used in a bagged decision tree as opposed to the random forest which makes a selection of the features at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Less Variance due to less complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612.7976878399319"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_svr = mean_squared_error(y_test, svr_preds)\n",
    "\n",
    "MSE_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22319366506252677"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_knn = mean_squared_error(y_test, knn_preds)\n",
    "\n",
    "MSE_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03436396824062117"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_bag = mean_squared_error(y_test, bag_preds)\n",
    "\n",
    "MSE_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.47549025929648"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_lr = mean_squared_error(y_test, lr_preds)\n",
    "\n",
    "MSE_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04067810031047903"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_rfr = mean_squared_error(y_test, rfr_preds)\n",
    "\n",
    "MSE_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.672423249799138"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_ada = mean_squared_error(y_test, ada_preds)\n",
    "\n",
    "MSE_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "No, I think most of these models had generally poor performance overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I wasn't surprised to find KNN had scored the best.  KNN on has good performance on these smaller data sets and the computational cost is relatively neglegable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Better engineer the data to find points that are more effeicient at the regression metrics.  This seems the be thebiggest issue currently.  Potentially more data would help in this endeavour aswell, simply put, I would work to gather more conculsive data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('e401k', axis = 1)\n",
    "y = df['e401k']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a $k$-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc.score(X_test,y_test)\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ada = AdaBoostClassifier()\n",
    "Ada.fit(X_train,y_train)\n",
    "Ada.score(X_test,y_test)\n",
    "Ada_preds = Ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = BaggingClassifier()\n",
    "Bag.fit(X_train,y_train)\n",
    "Bag.score(X_test,y_test)\n",
    "Bag_preds = Bag.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_train,y_train)\n",
    "lg.score(X_test,y_test)\n",
    "lg_preds = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test,y_test)\n",
    "svc_preds = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test,y_test)\n",
    "knn_preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "False Positive = Someone uneligible for 401k but is predicted as not\n",
    "False Negative = Someone eligible for 401k but is pedcited as not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Likely the false negative have less of a long term effect as people will be making contributions to their savings believing they are not being matched, meaning more contributions directly to their savings.  However if someone is told they are uneligible for a 401k they would potentially contribute less to their savings expecting a partial match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Essentially is it a metric of the balance between accuracy and precision.  By optimizing for a certain metric above we need to understand the balance on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8060422960725075"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, rfc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8182386008744534"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, Ada_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8107121119902616"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,Bag_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819262038774234"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, lg_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016477857878475798"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, svc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47957371225577267"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, knn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "AdaBoost seems to have the strongest performance amoungst all the graphs.  The boosting method seems especially good for a dataset of this size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "More features would allow for a more complex model which I think is a major issue for the classifier.  Alternatively, getting some more data within the features available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regression: Age, Marriage and Fsize\n",
    "Classification: (Note Above Models.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
